{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2C7AE8hvcyX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0ed90a8-de0f-47e9-fd34-3fc7f83e4da5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dogs-vs-cats-redux-kernels-edition.zip to /content\n",
            " 99% 808M/814M [00:04<00:00, 162MB/s]\n",
            "100% 814M/814M [00:04<00:00, 204MB/s]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/'\n",
        "!chmod 600 /content/kaggle.json\n",
        "!kaggle competitions download -c dogs-vs-cats-redux-kernels-edition -p /content --force"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q dogs-vs-cats-redux-kernels-edition.zip -d ."
      ],
      "metadata": {
        "id": "JIBqk2xFxtnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q train.zip -d ."
      ],
      "metadata": {
        "id": "7umw6lJcyISg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import shutil\n",
        "\n",
        "print(len(os.listdir('/content/train/')))\n",
        "\n",
        "#classify data\n",
        "for i in os.listdir('/content/train/'):\n",
        "  if 'cat' in i:\n",
        "    shutil.copyfile('/content/train/'+i, '/content/dataset/cats/' +i)\n",
        "  if 'dog' in i:\n",
        "    shutil.copyfile('/content/train/'+i, '/content/dataset/dogs/' +i)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJ6W64KxJnGY",
        "outputId": "d3f1369c-f710-4c24-f29a-089950bd5d17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#80% of data will be used for training\n",
        "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    '/content/dataset/',\n",
        "    image_size=(64,64),\n",
        "    batch_size = 64,\n",
        "    subset= 'training',\n",
        "    validation_split = 0.2,\n",
        "    seed = 1234\n",
        ")\n",
        "#20% of data will be used for validation\n",
        "val_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    '/content/dataset/',\n",
        "    image_size=(64,64),\n",
        "    batch_size = 64,\n",
        "    subset= 'validation',\n",
        "    validation_split = 0.2    ,\n",
        "    seed = 1234\n",
        ")\n",
        "\n",
        "#make all input data are 0 ~ 1\n",
        "def preprocess(i, answer):\n",
        "  i = tf.cast(i/255.0, tf.float32)\n",
        "  return i, answer\n",
        "\n",
        "train_dataset = train_dataset.map(preprocess)\n",
        "val_dataset = val_dataset.map(preprocess)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erEzReFzNfI9",
        "outputId": "cde14a2c-4eba-4d5a-b18d-496dce56b138"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal', input_shape = (64,64,3) ), #image augmentation\n",
        "    tf.keras.layers.experimental.preprocessing.RandomRotation(0.1),\n",
        "    tf.keras.layers.experimental.preprocessing.RandomZoom(0.1),\n",
        "\n",
        "    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2,2)),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2,2)),\n",
        "    tf.keras.layers.Dropout(0,2), #prevent overfitting\n",
        "    tf.keras.layers.Conv2D(128, (3,3), padding='same', activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2,2)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0,2),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid') # output will 0~1\n",
        "])\n",
        "\n",
        "model.compile(loss =\"binary_crossentropy\", optimizer = 'adam', metrics=['accuracy'])\n",
        "model.fit(train_dataset, validation_data= val_dataset, epochs=5)\n"
      ],
      "metadata": {
        "id": "eETAFTjVQ-hd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}